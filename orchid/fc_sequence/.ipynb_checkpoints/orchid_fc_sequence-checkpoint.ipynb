{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "15247b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from PIL import Image\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "26e663fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "# np.random.seed(1)\n",
    "# torch.manual_seed(1)\n",
    "# torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ea68223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "histwindow = 1\n",
    "batchsize = 200\n",
    "testsize = 10\n",
    "train_size = 1000\n",
    "epochs = 300\n",
    "# allsensors = ('sensor1',  'sensor2', 'sensor3','sensor4', 'sensor5', \n",
    "#               'sensor6', 'sensor8', 'sensor10', 'sensor12', 'sensor13','sensor14', 'sensor15',\n",
    "#              'sensor16',  'sensor17', 'sensor18', 'sensor19', 'sensor20', 'sensor21')\n",
    "allsensors = ( 'sensor5', 'sensor8', 'sensor9', 'sensor16', 'sensor18')\n",
    "fpath = '/proj/NARO/ghheatmap/orchid/refined_images_orchid'#ヒートマップのパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "395d6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsize = histwindow * len(allsensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cd8d83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = {}\n",
    "def sensdata(name):\n",
    "    jso = open(\"/proj/NARO/ghheatmap/orchid/sensor_data/{}/all.json\".format(name))#センサのパス\n",
    "    jsl = json.load(jso)\n",
    "    alldata[name] = jsl\n",
    "for sname in allsensors:\n",
    "    sensdata(sname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "df766eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(fpath+'/*.jpg')\n",
    "sdata = []\n",
    "fdata = []\n",
    "tdata = []\n",
    "for file in files:\n",
    "    reres = re.findall('.*/data_(20\\d\\d)(\\d\\d)(\\d\\d)_(\\d\\d)(\\d\\d)(\\d\\d).jpg', file)\n",
    "    (year, month, day, hour, minute, sec) = reres[0]\n",
    "    dt_date = datetime(int(year), int(month), int(day), int(hour), int(minute), int(sec))\n",
    "    dt_epoch = int((dt_date.timestamp()+30)/60)*60\n",
    "    dt_adj = datetime.fromtimestamp(dt_epoch)\n",
    "    dt_prev = dt_adj\n",
    "    time_adj = dt_adj.strftime('%Y-%m-%d-%H-%M')\n",
    "    time_prev = time_adj\n",
    "    sensd = []\n",
    "    histerr = 0\n",
    "    for hist in range(0, histwindow):\n",
    "        for sname in allsensors:\n",
    "            if alldata[sname].get(time_prev):\n",
    "                sensd.append(alldata[sname][time_prev]['Temperature'])\n",
    "\n",
    "            else:\n",
    "                histerr = 1\n",
    "                break\n",
    "        if histerr:\n",
    "            break\n",
    "        dt_prev = dt_prev - timedelta(minutes=1)\n",
    "        time_prev = dt_prev.strftime('%Y-%m-%d-%H-%M')\n",
    "    if not histerr:\n",
    "        sdata.append(sensd)\n",
    "        tdata.append(time_adj)\n",
    "        fdata.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b6cbe08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        self.data_num = len(tdata)\n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(fdata[idx])\n",
    "#        if self.transform:\n",
    "#            out_data = self.transform(image)\n",
    "        out_data = np.array(image).reshape(16*18).astype(np.int32) \n",
    "        out_data = out_data*50/255\n",
    "\n",
    "        return torch.Tensor(sdata[idx]), torch.Tensor(out_data) \n",
    "data_set = MyDataset()\n",
    "print(data_set.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cb2096f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Subset\n",
    "def split_dataset(data_set, split_at, order=None):\n",
    "    from torch.utils.data.dataset import Subset\n",
    "    n_examples = len(data_set)\n",
    "    if split_at < 0:\n",
    "        raise ValueError('split_at must be non-negative')\n",
    "    if split_at > n_examples:\n",
    "        raise ValueError('split_at exceeds the dataset size')\n",
    "    if order is not None:\n",
    "        subset1_indices = order[0:split_at]\n",
    "        subset2_indices = order[split_at:n_examples]\n",
    "    else:\n",
    "        subset1_indices = list(range(0,split_at))\n",
    "        subset2_indices = list(range(split_at,n_examples))\n",
    "\n",
    "    subset1 = Subset(data_set, subset1_indices)\n",
    "    subset2 = Subset(data_set, subset2_indices)\n",
    "    return subset1, subset2\n",
    "\n",
    "def split_dataset_random(data_set, first_size, seed=0):\n",
    "    order = np.random.RandomState(seed).permutation(len(data_set))\n",
    "    return split_dataset(data_set, int(first_size), order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b24b3426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset 10\n",
      "train dataset 1000\n",
      "val dataset 201\n"
     ]
    }
   ],
   "source": [
    "test_dataset, data_set = split_dataset_random(data_set, testsize, seed=0)\n",
    "train_dataset, val_dataset = split_dataset_random(data_set, train_size, seed=0)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=len(val_dataset), shuffle=False)\n",
    "print('test dataset', len(test_dataset))\n",
    "print('train dataset', len(train_dataset))\n",
    "print('val dataset', len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "85f9d210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NET(\n",
       "  (fc1): Linear(in_features=5, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=288, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NET, self).__init__()\n",
    "        self.fc1 = nn.Linear(xsize, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 16*18)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = NET().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bb79b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 199.3550 val_loss: 74.8754\n",
      "epoch 1, loss: 57.0036 val_loss: 28.5870\n",
      "epoch 2, loss: 37.9791 val_loss: 14.4691\n",
      "epoch 3, loss: 20.1824 val_loss: 14.4749\n",
      "epoch 4, loss: 17.1226 val_loss: 9.7982\n",
      "epoch 5, loss: 14.4716 val_loss: 11.6650\n",
      "epoch 6, loss: 13.8789 val_loss: 9.6558\n",
      "epoch 7, loss: 9.9273 val_loss: 17.0204\n",
      "epoch 8, loss: 12.4968 val_loss: 14.9502\n",
      "epoch 9, loss: 9.8453 val_loss: 15.7053\n",
      "epoch 10, loss: 9.8021 val_loss: 17.0085\n",
      "epoch 11, loss: 12.0224 val_loss: 9.6669\n",
      "epoch 12, loss: 11.4758 val_loss: 12.2721\n",
      "epoch 13, loss: 12.5254 val_loss: 22.0127\n",
      "epoch 14, loss: 14.4517 val_loss: 22.2697\n",
      "epoch 15, loss: 15.3689 val_loss: 19.6871\n",
      "epoch 16, loss: 15.5737 val_loss: 14.8169\n",
      "epoch 17, loss: 12.9167 val_loss: 7.4368\n",
      "epoch 18, loss: 9.4490 val_loss: 9.1799\n",
      "epoch 19, loss: 11.0908 val_loss: 13.7134\n",
      "epoch 20, loss: 12.3342 val_loss: 7.3894\n",
      "epoch 21, loss: 8.6246 val_loss: 7.9549\n",
      "epoch 22, loss: 7.9586 val_loss: 6.6891\n",
      "epoch 23, loss: 7.3257 val_loss: 11.4146\n",
      "epoch 24, loss: 8.6732 val_loss: 6.3630\n",
      "epoch 25, loss: 5.5861 val_loss: 7.4890\n",
      "epoch 26, loss: 6.3944 val_loss: 5.8994\n",
      "epoch 27, loss: 5.4144 val_loss: 7.7820\n",
      "epoch 28, loss: 5.8050 val_loss: 7.9730\n",
      "epoch 29, loss: 5.8996 val_loss: 8.2944\n",
      "epoch 30, loss: 6.8126 val_loss: 25.9166\n",
      "epoch 31, loss: 14.6905 val_loss: 7.1246\n",
      "epoch 32, loss: 10.1666 val_loss: 24.5631\n",
      "epoch 33, loss: 14.4147 val_loss: 19.1092\n",
      "epoch 34, loss: 9.5936 val_loss: 6.7038\n",
      "epoch 35, loss: 6.5839 val_loss: 8.1687\n",
      "epoch 36, loss: 5.7134 val_loss: 7.0707\n",
      "epoch 37, loss: 6.4594 val_loss: 5.3620\n",
      "epoch 38, loss: 5.3137 val_loss: 5.1255\n",
      "epoch 39, loss: 4.9250 val_loss: 7.5050\n",
      "epoch 40, loss: 8.6062 val_loss: 12.3841\n",
      "epoch 41, loss: 12.1142 val_loss: 46.1069\n",
      "epoch 42, loss: 25.9791 val_loss: 11.6027\n",
      "epoch 43, loss: 6.9484 val_loss: 5.1843\n",
      "epoch 44, loss: 5.4688 val_loss: 6.8868\n",
      "epoch 45, loss: 6.0291 val_loss: 4.4608\n",
      "epoch 46, loss: 6.4358 val_loss: 5.2858\n",
      "epoch 47, loss: 4.2694 val_loss: 4.3314\n",
      "epoch 48, loss: 4.0684 val_loss: 8.9487\n",
      "epoch 49, loss: 5.1167 val_loss: 15.9172\n",
      "epoch 50, loss: 7.1749 val_loss: 6.9361\n",
      "epoch 51, loss: 7.6231 val_loss: 19.6603\n",
      "epoch 52, loss: 14.6123 val_loss: 10.2049\n",
      "epoch 53, loss: 7.5931 val_loss: 4.9286\n",
      "epoch 54, loss: 5.8338 val_loss: 4.6773\n",
      "epoch 55, loss: 8.6681 val_loss: 13.9866\n",
      "epoch 56, loss: 15.4311 val_loss: 12.2927\n",
      "epoch 57, loss: 12.1565 val_loss: 6.1071\n",
      "epoch 58, loss: 5.2064 val_loss: 13.1195\n",
      "epoch 59, loss: 7.6697 val_loss: 5.9082\n",
      "epoch 60, loss: 5.2597 val_loss: 6.0209\n",
      "epoch 61, loss: 5.5973 val_loss: 4.2267\n",
      "epoch 62, loss: 4.0123 val_loss: 7.6006\n",
      "epoch 63, loss: 5.6108 val_loss: 8.0605\n",
      "epoch 64, loss: 6.7564 val_loss: 3.2513\n",
      "epoch 65, loss: 3.0584 val_loss: 4.0215\n",
      "epoch 66, loss: 2.9579 val_loss: 5.4510\n",
      "epoch 67, loss: 4.4264 val_loss: 2.6316\n",
      "epoch 68, loss: 3.1407 val_loss: 4.4123\n",
      "epoch 69, loss: 3.9801 val_loss: 6.4456\n",
      "epoch 70, loss: 4.8694 val_loss: 4.0598\n",
      "epoch 71, loss: 2.6820 val_loss: 2.3509\n",
      "epoch 72, loss: 2.1165 val_loss: 2.8852\n",
      "epoch 73, loss: 2.5750 val_loss: 2.2088\n",
      "epoch 74, loss: 3.6675 val_loss: 11.2423\n",
      "epoch 75, loss: 7.0019 val_loss: 6.6495\n",
      "epoch 76, loss: 6.0776 val_loss: 2.5443\n",
      "epoch 77, loss: 3.1604 val_loss: 8.5407\n",
      "epoch 78, loss: 5.0286 val_loss: 2.2340\n",
      "epoch 79, loss: 2.7857 val_loss: 2.1956\n",
      "epoch 80, loss: 2.0923 val_loss: 2.1482\n",
      "epoch 81, loss: 1.9445 val_loss: 2.2797\n",
      "epoch 82, loss: 1.9609 val_loss: 2.7548\n",
      "epoch 83, loss: 1.9490 val_loss: 2.8390\n",
      "epoch 84, loss: 1.7843 val_loss: 1.6591\n",
      "epoch 85, loss: 1.5963 val_loss: 1.5458\n",
      "epoch 86, loss: 1.5476 val_loss: 1.8131\n",
      "epoch 87, loss: 1.5702 val_loss: 1.4406\n",
      "epoch 88, loss: 1.4764 val_loss: 4.1074\n",
      "epoch 89, loss: 2.2799 val_loss: 1.8861\n",
      "epoch 90, loss: 1.7298 val_loss: 1.6109\n",
      "epoch 91, loss: 1.4430 val_loss: 3.5022\n",
      "epoch 92, loss: 3.2421 val_loss: 10.4185\n",
      "epoch 93, loss: 8.2405 val_loss: 6.2661\n",
      "epoch 94, loss: 4.6266 val_loss: 3.2362\n",
      "epoch 95, loss: 3.0647 val_loss: 4.8393\n",
      "epoch 96, loss: 3.4104 val_loss: 6.2990\n",
      "epoch 97, loss: 4.2780 val_loss: 2.5216\n",
      "epoch 98, loss: 2.1057 val_loss: 1.8691\n",
      "epoch 99, loss: 1.6272 val_loss: 1.6309\n",
      "epoch 100, loss: 1.6773 val_loss: 4.6778\n",
      "epoch 101, loss: 2.8352 val_loss: 6.3158\n",
      "epoch 102, loss: 3.0303 val_loss: 1.4315\n",
      "epoch 103, loss: 1.6967 val_loss: 1.7023\n",
      "epoch 104, loss: 1.8062 val_loss: 1.4354\n",
      "epoch 105, loss: 1.2105 val_loss: 1.8088\n",
      "epoch 106, loss: 1.2317 val_loss: 3.2431\n",
      "epoch 107, loss: 2.5217 val_loss: 1.9059\n",
      "epoch 108, loss: 2.5953 val_loss: 2.6452\n",
      "epoch 109, loss: 1.5984 val_loss: 1.9810\n",
      "epoch 110, loss: 2.0483 val_loss: 1.8356\n",
      "epoch 111, loss: 1.8014 val_loss: 3.9072\n",
      "epoch 112, loss: 2.0563 val_loss: 3.5460\n",
      "epoch 113, loss: 2.4168 val_loss: 1.0451\n",
      "epoch 114, loss: 1.6983 val_loss: 0.9557\n",
      "epoch 115, loss: 0.9626 val_loss: 0.8995\n",
      "epoch 116, loss: 0.8553 val_loss: 1.5820\n",
      "epoch 117, loss: 1.2519 val_loss: 4.1232\n",
      "epoch 118, loss: 1.9203 val_loss: 7.4048\n",
      "epoch 119, loss: 3.6949 val_loss: 1.2733\n",
      "epoch 120, loss: 1.2973 val_loss: 3.2203\n",
      "epoch 121, loss: 1.5564 val_loss: 2.4414\n",
      "epoch 122, loss: 1.6551 val_loss: 0.9886\n",
      "epoch 123, loss: 1.4358 val_loss: 1.6603\n",
      "epoch 124, loss: 1.2666 val_loss: 5.8544\n",
      "epoch 125, loss: 2.6028 val_loss: 1.7259\n",
      "epoch 126, loss: 1.2849 val_loss: 3.8440\n",
      "epoch 127, loss: 1.8968 val_loss: 3.6512\n",
      "epoch 128, loss: 1.9670 val_loss: 1.8183\n",
      "epoch 129, loss: 1.1728 val_loss: 1.1034\n",
      "epoch 130, loss: 0.9869 val_loss: 0.6907\n",
      "epoch 131, loss: 0.9620 val_loss: 1.4927\n",
      "epoch 132, loss: 1.2823 val_loss: 1.0442\n",
      "epoch 133, loss: 0.8441 val_loss: 0.6738\n",
      "epoch 134, loss: 0.6297 val_loss: 0.6190\n",
      "epoch 135, loss: 0.6253 val_loss: 0.5833\n",
      "epoch 136, loss: 0.5713 val_loss: 0.6451\n",
      "epoch 137, loss: 0.5784 val_loss: 1.3737\n",
      "epoch 138, loss: 0.7590 val_loss: 1.5578\n",
      "epoch 139, loss: 0.9950 val_loss: 4.7308\n",
      "epoch 140, loss: 2.5920 val_loss: 0.5437\n",
      "epoch 141, loss: 1.2205 val_loss: 0.9689\n",
      "epoch 142, loss: 1.2436 val_loss: 1.5650\n",
      "epoch 143, loss: 1.2377 val_loss: 0.7892\n",
      "epoch 144, loss: 0.6598 val_loss: 0.6465\n",
      "epoch 145, loss: 0.5585 val_loss: 0.5655\n",
      "epoch 146, loss: 0.6504 val_loss: 0.7398\n",
      "epoch 147, loss: 0.6891 val_loss: 0.7206\n",
      "epoch 148, loss: 0.6337 val_loss: 1.5648\n",
      "epoch 149, loss: 1.0090 val_loss: 1.2173\n",
      "epoch 150, loss: 0.9832 val_loss: 0.4768\n",
      "epoch 151, loss: 0.6039 val_loss: 0.8181\n",
      "epoch 152, loss: 0.7607 val_loss: 2.0260\n",
      "epoch 153, loss: 1.3529 val_loss: 1.4274\n",
      "epoch 154, loss: 0.7884 val_loss: 1.2417\n",
      "epoch 155, loss: 0.6796 val_loss: 1.2940\n",
      "epoch 156, loss: 0.7663 val_loss: 1.5865\n",
      "epoch 157, loss: 1.0142 val_loss: 2.4149\n",
      "epoch 158, loss: 1.6014 val_loss: 0.6960\n",
      "epoch 159, loss: 0.6043 val_loss: 0.6130\n",
      "epoch 160, loss: 0.6258 val_loss: 0.5533\n",
      "epoch 161, loss: 0.5293 val_loss: 0.6130\n",
      "epoch 162, loss: 0.6037 val_loss: 0.5985\n",
      "epoch 163, loss: 0.6110 val_loss: 0.4995\n",
      "epoch 164, loss: 0.4738 val_loss: 0.4150\n",
      "epoch 165, loss: 0.3662 val_loss: 0.4114\n",
      "epoch 166, loss: 0.3753 val_loss: 0.3814\n",
      "epoch 167, loss: 0.3360 val_loss: 0.4259\n",
      "epoch 168, loss: 0.3731 val_loss: 0.4320\n",
      "epoch 169, loss: 0.5094 val_loss: 0.4933\n",
      "epoch 170, loss: 0.4342 val_loss: 0.5855\n",
      "epoch 171, loss: 0.4963 val_loss: 0.5468\n",
      "epoch 172, loss: 0.4776 val_loss: 0.3751\n",
      "epoch 173, loss: 0.4280 val_loss: 0.6361\n",
      "epoch 174, loss: 0.5674 val_loss: 0.4298\n",
      "epoch 175, loss: 0.4100 val_loss: 0.3909\n",
      "epoch 176, loss: 0.4264 val_loss: 0.4858\n",
      "epoch 177, loss: 0.3997 val_loss: 0.5735\n",
      "epoch 178, loss: 0.4888 val_loss: 0.3712\n",
      "epoch 179, loss: 0.3703 val_loss: 0.3534\n",
      "epoch 180, loss: 0.3480 val_loss: 0.3717\n",
      "epoch 181, loss: 0.3508 val_loss: 0.4034\n",
      "epoch 182, loss: 0.3358 val_loss: 0.3867\n",
      "epoch 183, loss: 0.3549 val_loss: 0.5378\n",
      "epoch 184, loss: 0.4997 val_loss: 0.3517\n",
      "epoch 185, loss: 0.5216 val_loss: 0.8105\n",
      "epoch 186, loss: 0.7292 val_loss: 0.3689\n",
      "epoch 187, loss: 0.4343 val_loss: 0.5495\n",
      "epoch 188, loss: 0.4862 val_loss: 0.4489\n",
      "epoch 189, loss: 0.4056 val_loss: 0.3941\n",
      "epoch 190, loss: 0.3887 val_loss: 0.5531\n",
      "epoch 191, loss: 0.5191 val_loss: 1.0223\n",
      "epoch 192, loss: 0.5315 val_loss: 0.6051\n",
      "epoch 193, loss: 0.4433 val_loss: 0.8036\n",
      "epoch 194, loss: 0.5926 val_loss: 0.5952\n",
      "epoch 195, loss: 0.4689 val_loss: 0.4963\n",
      "epoch 196, loss: 0.4183 val_loss: 0.4531\n",
      "epoch 197, loss: 0.3756 val_loss: 0.3584\n",
      "epoch 198, loss: 0.3289 val_loss: 0.3428\n",
      "epoch 199, loss: 0.3052 val_loss: 0.3160\n",
      "epoch 200, loss: 0.3419 val_loss: 0.4337\n",
      "epoch 201, loss: 0.4149 val_loss: 0.6603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 202, loss: 0.4824 val_loss: 0.6857\n",
      "epoch 203, loss: 0.5399 val_loss: 0.3395\n",
      "epoch 204, loss: 0.3101 val_loss: 0.3152\n",
      "epoch 205, loss: 0.3529 val_loss: 0.3338\n",
      "epoch 206, loss: 0.3365 val_loss: 0.3413\n",
      "epoch 207, loss: 0.3503 val_loss: 0.3600\n",
      "epoch 208, loss: 0.3129 val_loss: 0.3346\n",
      "epoch 209, loss: 0.2921 val_loss: 0.2955\n",
      "epoch 210, loss: 0.2690 val_loss: 0.3000\n",
      "epoch 211, loss: 0.2907 val_loss: 0.3662\n",
      "epoch 212, loss: 0.3655 val_loss: 0.2902\n",
      "epoch 213, loss: 0.3478 val_loss: 0.3400\n",
      "epoch 214, loss: 0.3181 val_loss: 0.3135\n",
      "epoch 215, loss: 0.2801 val_loss: 0.3228\n",
      "epoch 216, loss: 0.3177 val_loss: 0.2845\n",
      "epoch 217, loss: 0.3964 val_loss: 0.8757\n",
      "epoch 218, loss: 0.6880 val_loss: 0.3149\n",
      "epoch 219, loss: 0.3832 val_loss: 0.9990\n",
      "epoch 220, loss: 0.6658 val_loss: 0.6575\n",
      "epoch 221, loss: 0.4884 val_loss: 1.0459\n",
      "epoch 222, loss: 0.6801 val_loss: 0.4597\n",
      "epoch 223, loss: 0.5796 val_loss: 0.4150\n",
      "epoch 224, loss: 0.5686 val_loss: 0.3217\n",
      "epoch 225, loss: 0.4134 val_loss: 0.3025\n",
      "epoch 226, loss: 0.3492 val_loss: 0.7732\n",
      "epoch 227, loss: 0.5078 val_loss: 0.3586\n",
      "epoch 228, loss: 0.3955 val_loss: 0.3391\n",
      "epoch 229, loss: 0.3704 val_loss: 0.3270\n",
      "epoch 230, loss: 0.2728 val_loss: 0.3137\n",
      "epoch 231, loss: 0.2784 val_loss: 0.2964\n",
      "epoch 232, loss: 0.2684 val_loss: 0.3342\n",
      "epoch 233, loss: 0.2781 val_loss: 0.2850\n",
      "epoch 234, loss: 0.2562 val_loss: 0.4547\n",
      "epoch 235, loss: 0.3035 val_loss: 0.5339\n",
      "epoch 236, loss: 0.3236 val_loss: 0.4658\n",
      "epoch 237, loss: 0.3384 val_loss: 0.2771\n",
      "epoch 238, loss: 0.2610 val_loss: 0.2649\n",
      "epoch 239, loss: 0.2614 val_loss: 0.2806\n",
      "epoch 240, loss: 0.3144 val_loss: 0.2653\n",
      "epoch 241, loss: 0.2379 val_loss: 0.2692\n",
      "epoch 242, loss: 0.2595 val_loss: 0.2923\n",
      "epoch 243, loss: 0.2619 val_loss: 0.2603\n",
      "epoch 244, loss: 0.2703 val_loss: 0.2677\n",
      "epoch 245, loss: 0.2707 val_loss: 0.2534\n",
      "epoch 246, loss: 0.2440 val_loss: 0.2582\n",
      "epoch 247, loss: 0.2460 val_loss: 0.2787\n",
      "epoch 248, loss: 0.3204 val_loss: 0.3936\n",
      "epoch 249, loss: 0.3926 val_loss: 0.3353\n",
      "epoch 250, loss: 0.3628 val_loss: 0.2860\n",
      "epoch 251, loss: 0.2606 val_loss: 0.2663\n",
      "epoch 252, loss: 0.2648 val_loss: 0.2558\n",
      "epoch 253, loss: 0.2837 val_loss: 0.3883\n",
      "epoch 254, loss: 0.3090 val_loss: 0.2700\n",
      "epoch 255, loss: 0.3183 val_loss: 0.4247\n",
      "epoch 256, loss: 0.3275 val_loss: 0.2989\n",
      "epoch 257, loss: 0.2700 val_loss: 0.2878\n",
      "epoch 258, loss: 0.2672 val_loss: 0.2708\n",
      "epoch 259, loss: 0.2628 val_loss: 0.3101\n",
      "epoch 260, loss: 0.3218 val_loss: 0.3286\n",
      "epoch 261, loss: 0.3312 val_loss: 0.2853\n",
      "epoch 262, loss: 0.2888 val_loss: 0.2708\n",
      "epoch 263, loss: 0.2626 val_loss: 0.2484\n",
      "epoch 264, loss: 0.2545 val_loss: 0.2541\n",
      "epoch 265, loss: 0.3100 val_loss: 0.3333\n",
      "epoch 266, loss: 0.3305 val_loss: 0.3603\n",
      "epoch 267, loss: 0.3009 val_loss: 0.2936\n",
      "epoch 268, loss: 0.3215 val_loss: 0.5863\n",
      "epoch 269, loss: 0.4612 val_loss: 0.2684\n",
      "epoch 270, loss: 0.2771 val_loss: 0.2968\n",
      "epoch 271, loss: 0.2914 val_loss: 0.3678\n",
      "epoch 272, loss: 0.3246 val_loss: 0.2535\n",
      "epoch 273, loss: 0.3018 val_loss: 0.2488\n",
      "epoch 274, loss: 0.3075 val_loss: 0.2538\n",
      "epoch 275, loss: 0.2971 val_loss: 0.2585\n",
      "epoch 276, loss: 0.2896 val_loss: 0.5578\n",
      "epoch 277, loss: 0.3954 val_loss: 0.2842\n",
      "epoch 278, loss: 0.3069 val_loss: 0.2520\n",
      "epoch 279, loss: 0.2743 val_loss: 0.2525\n",
      "epoch 280, loss: 0.2780 val_loss: 0.2578\n",
      "epoch 281, loss: 0.2421 val_loss: 0.2843\n",
      "epoch 282, loss: 0.2481 val_loss: 0.2629\n",
      "epoch 283, loss: 0.2661 val_loss: 0.2457\n",
      "epoch 284, loss: 0.2703 val_loss: 0.2447\n",
      "epoch 285, loss: 0.3374 val_loss: 0.2659\n",
      "epoch 286, loss: 0.3753 val_loss: 0.2593\n",
      "epoch 287, loss: 0.3307 val_loss: 0.3344\n",
      "epoch 288, loss: 0.2978 val_loss: 0.3366\n",
      "epoch 289, loss: 0.4233 val_loss: 0.3163\n",
      "epoch 290, loss: 0.3223 val_loss: 0.3183\n",
      "epoch 291, loss: 0.2598 val_loss: 0.3125\n",
      "epoch 292, loss: 0.2875 val_loss: 0.2647\n",
      "epoch 293, loss: 0.2393 val_loss: 0.2538\n",
      "epoch 294, loss: 0.2243 val_loss: 0.2534\n",
      "epoch 295, loss: 0.2270 val_loss: 0.2413\n",
      "epoch 296, loss: 0.2138 val_loss: 0.2354\n",
      "epoch 297, loss: 0.2341 val_loss: 0.2537\n",
      "epoch 298, loss: 0.2835 val_loss: 0.3136\n",
      "epoch 299, loss: 0.3137 val_loss: 0.3431\n",
      "best_epoch 296\n",
      "経過時間：129.68805837631226\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "t1 = time.time()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_running_loss = 0\n",
    "    for batch_idx, (x, c) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        c = c.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = model(x)\n",
    "        train_loss = criterion(y, c)\n",
    "        train_running_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_loss = train_running_loss / len(train_loader) \n",
    "    \n",
    "    model.eval()\n",
    "    val_running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, c) in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            y = model(x)\n",
    "            val_loss = criterion(y, c)\n",
    "            val_running_loss += val_loss.item()\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'model.pth')\n",
    "        best_valid_loss = val_running_loss\n",
    "    else:\n",
    "        if best_valid_loss > val_running_loss:\n",
    "            torch.save(model.state_dict(), f\"model{epoch}.pth\")\n",
    "            best_valid_loss = val_running_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)  \n",
    "    print('epoch %d, loss: %.4f val_loss: %.4f' % (epoch, train_loss, val_loss))\n",
    "    \n",
    "    \n",
    "print('best_epoch', best_epoch)\n",
    "t2 = time.time()\n",
    "elapsed_time = t2-t1\n",
    "print(f\"経過時間：{elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6e544a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net = NET().to(torch.device(\"cpu\"))\n",
    "# test_net = NET().to(device)\n",
    "test_net.eval()\n",
    "test_net.load_state_dict(torch.load('model' + str(best_epoch) + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f0b254a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2599640/783524171.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_loss_list = torch.tensor(train_loss_list)\n",
      "/tmp/ipykernel_2599640/783524171.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_loss_list = torch.tensor(val_loss_list)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list = torch.tensor(train_loss_list)\n",
    "val_loss_list = torch.tensor(val_loss_list)\n",
    "\n",
    "plt.xlim(0, epochs)\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, label=\"Train\")\n",
    "plt.plot(range(len(val_loss_list)), val_loss_list, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1a7d5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "def test():\n",
    "    test_net.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        total_max_temp_error = 0\n",
    "        total_min_temp_error = 0\n",
    "        max_error_x = 0\n",
    "        max_error_y = 0\n",
    "        min_error_x = 0\n",
    "        min_error_y = 0\n",
    "        error_per_pix = 0\n",
    "        loss_sum = 0\n",
    "        max_error_x_abs = 0\n",
    "        max_error_y_abs = 0\n",
    "        min_error_x_abs = 0\n",
    "        min_error_y_abs = 0\n",
    "            \n",
    "        estimate_list = []\n",
    "\n",
    "        for batch_idx, (x, c) in enumerate(test_loader):\n",
    "            estimate_start = time.time()\n",
    "#             x = x.to(device)\n",
    "#             c = c.to(device)\n",
    "            y = None\n",
    "            y = model(x)\n",
    "            estimate_end = time.time()\n",
    "            estimate_duration = -estimate_start+estimate_end\n",
    "            estimate_list.append(estimate_duration)\n",
    "            loss = criterion(y, c)\n",
    "            loss_unit = torch.sum(torch.abs(y-c))/16/18\n",
    "            loss_sum = loss_sum + loss_unit\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            y = y.cpu()\n",
    "            c = c.cpu()\n",
    "            c_reshaped = c.reshape(16, 18)\n",
    "            seaborn.heatmap(c_reshaped, vmin=18, vmax=25)\n",
    "            plt.show()\n",
    "            y_reshaped = y.reshape(16, 18)\n",
    "            seaborn.heatmap(y_reshaped, vmin=18, vmax=25)\n",
    "            plt.show()\n",
    "            print('==================')\n",
    "            \n",
    "            #最高温度のずれ\n",
    "            max_temp_output = torch.max(c)\n",
    "            max_temp_target = torch.max(y)\n",
    "            max_temp_error = max_temp_output - max_temp_target\n",
    "            total_max_temp_error = total_max_temp_error + max_temp_error\n",
    "\n",
    "            #最低温度のずれ\n",
    "            min_temp_output = torch.min(c)\n",
    "            min_temp_target = torch.min(y)\n",
    "            min_temp_error = min_temp_output - min_temp_target\n",
    "            total_min_temp_error = total_min_temp_error + min_temp_error\n",
    "\n",
    "            #最高温度を示すピクセルの座標のずれ(絶対値)\n",
    "            max_temp_arg = torch.argmax(c)\n",
    "            max_x_output = (max_temp_arg + 1)%18 -1\n",
    "            max_y_output = -(max_temp_arg + 1)//18\n",
    "\n",
    "            max_temp_arg = torch.argmax(y)\n",
    "            max_x_target = (max_temp_arg + 1)%18 -1\n",
    "            max_y_target = -(max_temp_arg + 1)//18\n",
    "            \n",
    "            max_error_x = max_error_x + abs(max_x_output - max_x_target)\n",
    "            max_error_y = max_error_y + abs(max_y_output - max_y_target)\n",
    "\n",
    "            # 最低温度を示すピクセルの座標のずれ(絶対値)\n",
    "            min_temp_arg = torch.argmin(c)\n",
    "            min_x_output = (min_temp_arg + 1)%18 -1\n",
    "            min_y_output = -(min_temp_arg + 1)//18\n",
    "\n",
    "            min_temp_arg = torch.argmin(y)\n",
    "            min_x_target = (min_temp_arg + 1)%18 -1\n",
    "            min_y_target = -(min_temp_arg + 1)//18\n",
    "            \n",
    "            min_error_x = min_error_x + abs(min_x_output - min_x_target)\n",
    "            min_error_y = min_error_y + abs(min_y_output - min_y_target)\n",
    "\n",
    "    print(\"推定平均時間:{}秒\".format(sum(estimate_list)/len(estimate_list)))\n",
    "\n",
    "    print('1ピクセルあたりの誤差', loss_sum/len(test_loader))\n",
    "    print('最高温度のずれ', total_max_temp_error/len(test_loader))      \n",
    "    print('最低温度のずれ', total_min_temp_error/len(test_loader))\n",
    "    print('最高温度を示すピクセルの座標のずれ', max_error_x_abs/len(test_loader), max_error_y_abs/len(test_loader), )\n",
    "    print('最低温度を示すピクセルの座標のずれ', min_error_x_abs/len(test_loader), min_error_y_abs/len(test_loader), )\n",
    "    \n",
    "    print(\"batch_size: {}　\".format(batchsize))\n",
    "    print(fpath)\n",
    "    p = [loss_sum/len(test_loader), total_max_temp_error/len(test_loader), total_min_temp_error/len(test_loader), max_error_x/len(test_loader), max_error_y/len(test_loader), min_error_x/len(test_loader), min_error_y/len(test_loader), max_error_x_abs/len(test_loader), max_error_y_abs/len(test_loader), min_error_x_abs/len(test_loader), min_error_y_abs/len(test_loader), best_epoch]\n",
    "    p = [float(x) for x in p]\n",
    "    p.insert(0, histwindow)\n",
    "    print(*p, sep=', ')\n",
    "\n",
    "    val_loss = running_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6f0c4c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2599640/2881451904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallsensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' : '\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mallsensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mallsensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' : '\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mallsensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mallsensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2599640/358458294.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#             x = x.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#             c = c.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mestimate_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mestimate_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mestimate_start\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mestimate_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2599640/1031267116.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "test()\n",
    "print(allsensors[0][6:] + ' : '  + allsensors[1][6:] + ' : ' + allsensors[2][6:] + ' : '  + allsensors[3][6:] + ' : ' + allsensors[4][6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13710f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
