{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15247b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from PIL import Image\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e663fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628a2938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor15 sensor13 sensor14 sensor10 sensor2\n"
     ]
    }
   ],
   "source": [
    "SENSORS = [\"sensor{}\".format(i) for i in range(1, 22)]\n",
    "allsensors = random.sample(SENSORS, 5)\n",
    "print(*allsensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea68223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "histwindow = 5 \n",
    "batchsize = 200\n",
    "testsize = 10\n",
    "epochs_01 = 3\n",
    "epochs_001 = 200\n",
    "# allsensors = ('sensor10',  'sensor11', 'sensor15','sensor16', 'sensor20')\n",
    "fpath = '/proj/NARO/ghheatmap/orchid/refined_images_orchid'#ヒートマップのパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395d6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsize = histwindow * len(allsensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8d83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = {}\n",
    "def sensdata(name):\n",
    "    jso = open(\"/proj/NARO/ghheatmap/orchid/sensor_data/{}/all.json\".format(name))#センサのパス\n",
    "    jsl = json.load(jso)\n",
    "    alldata[name] = jsl\n",
    "for sname in allsensors:\n",
    "    sensdata(sname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df766eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(fpath+'/*.jpg')\n",
    "sdata = []\n",
    "fdata = []\n",
    "tdata = []\n",
    "for file in files:\n",
    "    reres = re.findall('.*/data_(20\\d\\d)(\\d\\d)(\\d\\d)_(\\d\\d)(\\d\\d)(\\d\\d).jpg', file)\n",
    "    (year, month, day, hour, minute, sec) = reres[0]\n",
    "    dt_date = datetime(int(year), int(month), int(day), int(hour), int(minute), int(sec))\n",
    "    dt_epoch = int((dt_date.timestamp()+30)/60)*60\n",
    "    dt_adj = datetime.fromtimestamp(dt_epoch)\n",
    "    dt_prev = dt_adj\n",
    "    time_adj = dt_adj.strftime('%Y-%m-%d-%H-%M')\n",
    "    time_prev = time_adj\n",
    "    sensd = []\n",
    "    histerr = 0\n",
    "    for hist in range(0, histwindow):\n",
    "        for sname in allsensors:\n",
    "            if alldata[sname].get(time_prev):\n",
    "                sensd.append(alldata[sname][time_prev]['Temperature'])\n",
    "            else:\n",
    "                histerr = 1\n",
    "                break\n",
    "        if histerr:\n",
    "            break\n",
    "        dt_prev = dt_prev - timedelta(minutes=1)\n",
    "        time_prev = dt_prev.strftime('%Y-%m-%d-%H-%M')\n",
    "    if not histerr:\n",
    "        sdata.append(sensd)\n",
    "        tdata.append(time_adj)\n",
    "        fdata.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6cbe08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        self.data_num = len(tdata)\n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(fdata[idx])\n",
    "#        if self.transform:\n",
    "#            out_data = self.transform(image)\n",
    "        out_data = np.array(image).reshape(16*18).astype(np.int32) \n",
    "        out_data = out_data*50/255\n",
    "        return torch.Tensor(sdata[idx]), torch.Tensor(out_data) \n",
    "data_set = MyDataset()\n",
    "print(len(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc41d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmapshow(data):\n",
    "    im = Image.fromarray(data[0].numpy().reshape([16,18]).copy()*255/50)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(im, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64dcf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797.6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(data_set)*0.8\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2096f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Subset\n",
    "def split_dataset(data_set, split_at, order=None):\n",
    "    from torch.utils.data.dataset import Subset\n",
    "    n_examples = len(data_set)\n",
    "    if split_at < 0:\n",
    "        raise ValueError('split_at must be non-negative')\n",
    "    if split_at > n_examples:\n",
    "        raise ValueError('split_at exceeds the dataset size')\n",
    "    if order is not None:\n",
    "        subset1_indices = order[0:split_at]\n",
    "        subset2_indices = order[split_at:n_examples]\n",
    "    else:\n",
    "        subset1_indices = list(range(0,split_at))\n",
    "        subset2_indices = list(range(split_at,n_examples))\n",
    "\n",
    "    subset1 = Subset(data_set, subset1_indices)\n",
    "    subset2 = Subset(data_set, subset2_indices)\n",
    "    return subset1, subset2\n",
    "\n",
    "def split_dataset_random(data_set, first_size, seed=0):\n",
    "    order = np.random.RandomState(seed).permutation(len(data_set))\n",
    "    return split_dataset(data_set, int(first_size), order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b24b3426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 977 10\n"
     ]
    }
   ],
   "source": [
    "test_dataset, data_set = split_dataset_random(data_set, 10, seed=0)\n",
    "val_dataset, train_dataset = split_dataset_random(data_set, 10, seed=0)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=batchsize, shuffle=False)\n",
    "print(len(test_dataset), len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f9d210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NET(\n",
       "  (fc1): Linear(in_features=25, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=288, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NET, self).__init__()\n",
    "        self.fc1 = nn.Linear(xsize, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 16*18)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = NET().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6de91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb79b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 34612834.3971 val_loss: 380.7830\n",
      "epoch 1, loss: 220.0411 val_loss: 3.9705\n",
      "epoch 2, loss: 57.8833 val_loss: 74.7146\n",
      "========== lr changed ============\n",
      "epoch 0, loss: 70.1078 val_loss: 63.2700\n",
      "epoch 1, loss: 58.8764 val_loss: 52.4223\n",
      "epoch 2, loss: 48.3203 val_loss: 42.3167\n",
      "epoch 3, loss: 38.5576 val_loss: 33.1049\n",
      "epoch 4, loss: 29.7593 val_loss: 24.9448\n",
      "epoch 5, loss: 22.0635 val_loss: 17.9633\n",
      "epoch 6, loss: 15.5856 val_loss: 12.2506\n",
      "epoch 7, loss: 10.3819 val_loss: 7.8290\n",
      "epoch 8, loss: 6.4718 val_loss: 4.6401\n",
      "epoch 9, loss: 3.7258 val_loss: 2.5533\n",
      "epoch 10, loss: 2.0123 val_loss: 1.3500\n",
      "epoch 11, loss: 1.0860 val_loss: 0.7776\n",
      "epoch 12, loss: 0.6756 val_loss: 0.5872\n",
      "epoch 13, loss: 0.5732 val_loss: 0.5724\n",
      "epoch 14, loss: 0.5863 val_loss: 0.6033\n",
      "epoch 15, loss: 0.6128 val_loss: 0.6203\n",
      "epoch 16, loss: 0.6181 val_loss: 0.6115\n",
      "epoch 17, loss: 0.6038 val_loss: 0.5908\n",
      "epoch 18, loss: 0.5826 val_loss: 0.5694\n",
      "epoch 19, loss: 0.5634 val_loss: 0.5553\n",
      "epoch 20, loss: 0.5524 val_loss: 0.5481\n",
      "epoch 21, loss: 0.5469 val_loss: 0.5459\n",
      "epoch 22, loss: 0.5469 val_loss: 0.5456\n",
      "epoch 23, loss: 0.5461 val_loss: 0.5458\n",
      "epoch 24, loss: 0.5457 val_loss: 0.5456\n",
      "epoch 25, loss: 0.5459 val_loss: 0.5452\n",
      "epoch 26, loss: 0.5453 val_loss: 0.5448\n",
      "epoch 27, loss: 0.5448 val_loss: 0.5446\n",
      "epoch 28, loss: 0.5457 val_loss: 0.5445\n",
      "epoch 29, loss: 0.5454 val_loss: 0.5444\n",
      "epoch 30, loss: 0.5452 val_loss: 0.5444\n",
      "epoch 31, loss: 0.5454 val_loss: 0.5445\n",
      "epoch 32, loss: 0.5453 val_loss: 0.5444\n",
      "epoch 33, loss: 0.5444 val_loss: 0.5444\n",
      "epoch 34, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 35, loss: 0.5444 val_loss: 0.5444\n",
      "epoch 36, loss: 0.5445 val_loss: 0.5444\n",
      "epoch 37, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 38, loss: 0.5445 val_loss: 0.5444\n",
      "epoch 39, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 40, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 41, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 42, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 43, loss: 0.5451 val_loss: 0.5444\n",
      "epoch 44, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 45, loss: 0.5440 val_loss: 0.5444\n",
      "epoch 46, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 47, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 48, loss: 0.5443 val_loss: 0.5444\n",
      "epoch 49, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 50, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 51, loss: 0.5439 val_loss: 0.5444\n",
      "epoch 52, loss: 0.5444 val_loss: 0.5444\n",
      "epoch 53, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 54, loss: 0.5451 val_loss: 0.5444\n",
      "epoch 55, loss: 0.5444 val_loss: 0.5444\n",
      "epoch 56, loss: 0.5445 val_loss: 0.5444\n",
      "epoch 57, loss: 0.5456 val_loss: 0.5444\n",
      "epoch 58, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 59, loss: 0.5442 val_loss: 0.5444\n",
      "epoch 60, loss: 0.5444 val_loss: 0.5444\n",
      "epoch 61, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 62, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 63, loss: 0.5451 val_loss: 0.5444\n",
      "epoch 64, loss: 0.5451 val_loss: 0.5444\n",
      "epoch 65, loss: 0.5446 val_loss: 0.5444\n",
      "epoch 66, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 67, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 68, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 69, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 70, loss: 0.5443 val_loss: 0.5444\n",
      "epoch 71, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 72, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 73, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 74, loss: 0.5441 val_loss: 0.5444\n",
      "epoch 75, loss: 0.5443 val_loss: 0.5444\n",
      "epoch 76, loss: 0.5452 val_loss: 0.5444\n",
      "epoch 77, loss: 0.5446 val_loss: 0.5444\n",
      "epoch 78, loss: 0.5442 val_loss: 0.5444\n",
      "epoch 79, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 80, loss: 0.5446 val_loss: 0.5444\n",
      "epoch 81, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 82, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 83, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 84, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 85, loss: 0.5453 val_loss: 0.5444\n",
      "epoch 86, loss: 0.5453 val_loss: 0.5444\n",
      "epoch 87, loss: 0.5446 val_loss: 0.5444\n",
      "epoch 88, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 89, loss: 0.5441 val_loss: 0.5444\n",
      "epoch 90, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 91, loss: 0.5444 val_loss: 0.5444\n",
      "epoch 92, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 93, loss: 0.5464 val_loss: 0.5444\n",
      "epoch 94, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 95, loss: 0.5441 val_loss: 0.5444\n",
      "epoch 96, loss: 0.5445 val_loss: 0.5444\n",
      "epoch 97, loss: 0.5452 val_loss: 0.5444\n",
      "epoch 98, loss: 0.5445 val_loss: 0.5444\n",
      "epoch 99, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 100, loss: 0.5453 val_loss: 0.5444\n",
      "epoch 101, loss: 0.5440 val_loss: 0.5444\n",
      "epoch 102, loss: 0.5446 val_loss: 0.5444\n",
      "epoch 103, loss: 0.5452 val_loss: 0.5444\n",
      "epoch 104, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 105, loss: 0.5457 val_loss: 0.5444\n",
      "epoch 106, loss: 0.5443 val_loss: 0.5444\n",
      "epoch 107, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 108, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 109, loss: 0.5451 val_loss: 0.5444\n",
      "epoch 110, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 111, loss: 0.5451 val_loss: 0.5444\n",
      "epoch 112, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 113, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 114, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 115, loss: 0.5451 val_loss: 0.5444\n",
      "epoch 116, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 117, loss: 0.5452 val_loss: 0.5444\n",
      "epoch 118, loss: 0.5461 val_loss: 0.5444\n",
      "epoch 119, loss: 0.5449 val_loss: 0.5444\n",
      "epoch 120, loss: 0.5450 val_loss: 0.5444\n",
      "epoch 121, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 122, loss: 0.5452 val_loss: 0.5444\n",
      "epoch 123, loss: 0.5448 val_loss: 0.5444\n",
      "epoch 124, loss: 0.5439 val_loss: 0.5444\n",
      "epoch 125, loss: 0.5438 val_loss: 0.5444\n",
      "epoch 126, loss: 0.5441 val_loss: 0.5444\n",
      "epoch 127, loss: 0.5439 val_loss: 0.5444\n",
      "epoch 128, loss: 0.5446 val_loss: 0.5444\n",
      "epoch 129, loss: 0.5447 val_loss: 0.5444\n",
      "epoch 130, loss: 0.5452 val_loss: 0.5444\n",
      "epoch 131, loss: 0.5441 val_loss: 0.5444\n",
      "epoch 132, loss: 0.5447 val_loss: 0.5445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40445/3453515640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtrain_running_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grad/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_40445/491495919.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#        if self.transform:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#            out_data = self.transform(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_data\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "t1 = time.time()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "for epoch in range(epochs_01):\n",
    "    model.train()\n",
    "    train_running_loss = 0\n",
    "    for batch_idx, (x, c) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        c = c.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = model(x)\n",
    "        train_loss = criterion(y, c)\n",
    "        train_running_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_loss = train_running_loss / len(train_loader) \n",
    "    \n",
    "    model.eval()\n",
    "    val_running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, c) in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            y = model(x)\n",
    "            val_loss = criterion(y, c)\n",
    "            val_running_loss += val_loss.item()\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'model.pth')\n",
    "        best_valid_loss = val_running_loss\n",
    "    else:\n",
    "        if best_valid_loss > val_running_loss:\n",
    "            torch.save(model.state_dict(), f\"model{epoch}.pth\")\n",
    "            best_valid_loss = val_running_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)  \n",
    "    print('epoch %d, loss: %.4f val_loss: %.4f' % (epoch, train_loss, val_loss))\n",
    "    \n",
    "    \n",
    "print('best_epoch', best_epoch)\n",
    "t2 = time.time()\n",
    "elapsed_time = t2-t1\n",
    "print(f\"経過時間：{elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e544a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = NET().to(device)\n",
    "test_net.eval()\n",
    "test_net.load_state_dict(torch.load('model' + str(best_epoch) + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b254a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = torch.tensor(train_loss_list)\n",
    "val_loss_list = torch.tensor(val_loss_list)\n",
    "\n",
    "plt.xlim(0, epochs_01)\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, label=\"Train\")\n",
    "plt.plot(range(len(val_loss_list)), val_loss_list, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "def test():\n",
    "    test_net.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        total_max_temp_error = 0\n",
    "        total_min_temp_error = 0\n",
    "        max_error_x = 0\n",
    "        max_error_y = 0\n",
    "        min_error_x = 0\n",
    "        min_error_y = 0\n",
    "        error_per_pix = 0\n",
    "        loss_sum = 0\n",
    "        max_error_x_abs = 0\n",
    "        max_error_y_abs = 0\n",
    "        min_error_x_abs = 0\n",
    "        min_error_y_abs = 0\n",
    "            \n",
    "        estimate_list = []\n",
    "\n",
    "        for batch_idx, (x, c) in enumerate(test_loader):\n",
    "            estimate_start = time.time()\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            y = model(x)\n",
    "            estimate_end = time.time()\n",
    "            estimate_duration = -estimate_start+estimate_end\n",
    "#             print(\"{}個目のモデルの推定時間は：{}秒です\".format(batch_idx+1, estimate_duration))\n",
    "            estimate_list.append(estimate_duration)\n",
    "            loss = criterion(y, c)\n",
    "            loss_unit = torch.sum(torch.abs(y-c))/16/18\n",
    "            loss_sum = loss_sum + loss_unit\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            y = y.cpu()\n",
    "            c = c.cpu()\n",
    "#             c_reshaped = c.reshape(16, 18)\n",
    "#             seaborn.heatmap(c_reshaped, vmin=18, vmax=25)\n",
    "#             plt.show()\n",
    "            y_reshaped = y.reshape(16, 18)\n",
    "            seaborn.heatmap(y_reshaped, vmin=18, vmax=25)\n",
    "            plt.show()\n",
    "            \n",
    "            #最高温度のずれ\n",
    "            max_temp_output = torch.max(c)\n",
    "            max_temp_target = torch.max(y)\n",
    "            max_temp_error = max_temp_output - max_temp_target\n",
    "            total_max_temp_error = total_max_temp_error + max_temp_error\n",
    "\n",
    "            #最低温度のずれ\n",
    "            min_temp_output = torch.min(c)\n",
    "            min_temp_target = torch.min(y)\n",
    "            min_temp_error = min_temp_output - min_temp_target\n",
    "            total_min_temp_error = total_min_temp_error + min_temp_error\n",
    "\n",
    "            #最高温度を示すピクセルの座標のずれ(絶対値)\n",
    "            max_temp_arg = torch.argmax(c)\n",
    "            max_x_output = (max_temp_arg + 1)%18 -1\n",
    "            max_y_output = -(max_temp_arg + 1)//18\n",
    "\n",
    "            max_temp_arg = torch.argmax(y)\n",
    "            max_x_target = (max_temp_arg + 1)%18 -1\n",
    "            max_y_target = -(max_temp_arg + 1)//18\n",
    "            \n",
    "            max_error_x = max_error_x + abs(max_x_output - max_x_target)\n",
    "            max_error_y = max_error_y + abs(max_y_output - max_y_target)\n",
    "\n",
    "            # 最低温度を示すピクセルの座標のずれ(絶対値)\n",
    "            min_temp_arg = torch.argmin(c)\n",
    "            min_x_output = (min_temp_arg + 1)%18 -1\n",
    "            min_y_output = -(min_temp_arg + 1)//18\n",
    "\n",
    "            min_temp_arg = torch.argmin(y)\n",
    "            min_x_target = (min_temp_arg + 1)%18 -1\n",
    "            min_y_target = -(min_temp_arg + 1)//18\n",
    "            \n",
    "            min_error_x = min_error_x + abs(min_x_output - min_x_target)\n",
    "            min_error_y = min_error_y + abs(min_y_output - min_y_target)\n",
    "\n",
    "    print(\"平均時間は{}秒です\".format(sum(estimate_list)/len(estimate_list)))\n",
    "\n",
    "    print('1ピクセルあたりの誤差', loss_sum/len(test_loader))\n",
    "    print('最高温度のずれ', total_max_temp_error/len(test_loader))      \n",
    "    print('最低温度のずれ', total_min_temp_error/len(test_loader))\n",
    "    print('最高温度を示すピクセルの座標のずれ', max_error_x_abs/len(test_loader), max_error_y_abs/len(test_loader), )\n",
    "    print('最低温度を示すピクセルの座標のずれ', min_error_x_abs/len(test_loader), min_error_y_abs/len(test_loader), )\n",
    "    \n",
    "    print(\"batch_size: {}　です。頑張ってください\".format(batchsize))\n",
    "    print(fpath)\n",
    "    p = [loss_sum/len(test_loader), total_max_temp_error/len(test_loader), total_min_temp_error/len(test_loader), max_error_x/len(test_loader), max_error_y/len(test_loader), min_error_x/len(test_loader), min_error_y/len(test_loader), max_error_x_abs/len(test_loader), max_error_y_abs/len(test_loader), min_error_x_abs/len(test_loader), min_error_y_abs/len(test_loader), best_epoch]\n",
    "    p = [float(x) for x in p]\n",
    "    p.insert(0, histwindow)\n",
    "    print(*p, sep=', ')\n",
    "\n",
    "    val_loss = running_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c4c08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test()\n",
    "print(allsensors[0][6:] + ' : '  + allsensors[1][6:] + ' : ' + allsensors[2][6:] + ' : '  + allsensors[3][6:] + ' : ' + allsensors[4][6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13710f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
