{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15247b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from PIL import Image\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e663fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea68223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "histwindow = 1 \n",
    "batchsize = 100\n",
    "testsize = 10\n",
    "epochs = 500\n",
    "allsensors = ('sensor1', 'sensor2', 'sensor3', 'sensor5', 'sensor6')\n",
    "fpath = '/proj/NARO/sonoda/noken/noken_0_50_selected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7080df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsize = histwindow * len(allsensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8d83b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../sonoda/sensor1/all.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5312/3320417428.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malldata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallsensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msensdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5312/3320417428.py\u001b[0m in \u001b[0;36msensdata\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msensdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mjso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../sonoda/{}/all.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mjsl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjso\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malldata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../sonoda/sensor1/all.json'"
     ]
    }
   ],
   "source": [
    "alldata = {}\n",
    "def sensdata(name):\n",
    "    jso = open(\"../sonoda/{}/all.json\".format(name))\n",
    "    jsl = json.load(jso)\n",
    "    alldata[name] = jsl\n",
    "for sname in allsensors:\n",
    "    sensdata(sname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df766eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(fpath+'/*.jpg')\n",
    "sdata = []\n",
    "fdata = []\n",
    "tdata = []\n",
    "\n",
    "for file in files:#全ヒートマップについて\n",
    "    reres = re.findall('.*/data_(20\\d\\d)(\\d\\d)(\\d\\d)_(\\d\\d)(\\d\\d)(\\d\\d).jpg', file)\n",
    "    (year, month, day, hour, minute, sec) = reres[0]\n",
    "    dt_date = datetime(int(year), int(month), int(day), int(hour), int(minute), int(sec))\n",
    "    dt_epoch = int((dt_date.timestamp()+30)/60)*60\n",
    "    dt_adj = datetime.fromtimestamp(dt_epoch)\n",
    "    dt_prev = dt_adj\n",
    "    time_adj = dt_adj.strftime('%Y-%m-%d-%H-%M')\n",
    "    time_prev = time_adj\n",
    "    sensd = []\n",
    "    histerr = 0\n",
    "    for hist in range(0, histwindow):\n",
    "        for sname in allsensors:\n",
    "            if alldata[sname].get(time_prev):\n",
    "                sensd.append(alldata[sname][time_prev]['Temperature'])\n",
    "            else:\n",
    "                histerr = 1\n",
    "                break\n",
    "        if histerr:\n",
    "            break\n",
    "        dt_prev = dt_prev - timedelta(minutes=1)\n",
    "        time_prev = dt_prev.strftime('%Y-%m-%d-%H-%M')\n",
    "    if not histerr:\n",
    "        sdata.append(sensd)\n",
    "        tdata.append(time_adj)\n",
    "        fdata.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cbe08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        self.data_num = len(tdata)\n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(fdata[idx])\n",
    "#        if self.transform:\n",
    "#            out_data = self.transform(image)\n",
    "        out_data = np.array(image).reshape(16*18).astype(np.int32)\n",
    "        out_data = out_data*50/255\n",
    "        return torch.Tensor(sdata[idx]), torch.Tensor(out_data) \n",
    "data_set = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmapshow(data):\n",
    "    im = Image.fromarray(data[0].numpy().reshape([16,18]).copy()*255/50)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(im, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64dcf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(data_set)*0.8\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2096f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Subset\n",
    "def split_dataset(data_set, split_at, order=None):\n",
    "    from torch.utils.data.dataset import Subset\n",
    "    n_examples = len(data_set)\n",
    "    if split_at < 0:\n",
    "        raise ValueError('split_at must be non-negative')\n",
    "    if split_at > n_examples:\n",
    "        raise ValueError('split_at exceeds the dataset size')\n",
    "    if order is not None:\n",
    "        subset1_indices = order[0:split_at]\n",
    "        subset2_indices = order[split_at:n_examples]\n",
    "    else:\n",
    "        subset1_indices = list(range(0,split_at))\n",
    "        subset2_indices = list(range(split_at,n_examples))\n",
    "\n",
    "    subset1 = Subset(data_set, subset1_indices)\n",
    "    subset2 = Subset(data_set, subset2_indices)\n",
    "    return subset1, subset2\n",
    "\n",
    "def split_dataset_random(data_set, first_size, seed=0):\n",
    "    order = np.random.RandomState(seed).permutation(len(data_set))\n",
    "    return split_dataset(data_set, int(first_size), order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset, data_set = split_dataset_random(data_set, testsize, seed=33)\n",
    "train_dataset, val_dataset = split_dataset_random(data_set, train_size, seed=0)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "train_loader = DataLoader(dataset=data_set, batch_size=batchsize, shuffle=True)\n",
    "val_loader = DataLoader(dataset=data_set, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f9d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NET, self).__init__()\n",
    "        self.fc1 = nn.Linear(xsize, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 16*18)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = NET().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6de91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    train_running_loss = 0\n",
    "    for batch_idx, (x, c) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        c = c.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = model(x)\n",
    "        train_loss = criterion(y, c)\n",
    "        train_running_loss += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_loss = train_running_loss / len(train_loader) \n",
    "    \n",
    "    model.eval()\n",
    "    val_running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, c) in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            y = model(x)\n",
    "            val_loss = criterion(y, c)\n",
    "            val_running_loss += val_loss.item()\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'model.pth')\n",
    "        best_valid_loss = val_running_loss\n",
    "    else:\n",
    "        if best_valid_loss > val_running_loss:\n",
    "            torch.save(model.state_dict(), f\"model{epoch}.pth\")\n",
    "            best_valid_loss = val_running_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)  \n",
    "    print('epoch %d, loss: %.4f val_loss: %.4f' % (epoch, train_loss, val_loss))\n",
    "    \n",
    "print('best_epoch', best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e544a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = NET().to(device)\n",
    "test_net.eval()\n",
    "test_net.load_state_dict(torch.load('model' + str(best_epoch) +'.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b254a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = torch.tensor(train_loss_list)\n",
    "val_loss_list = torch.tensor(val_loss_list)\n",
    "\n",
    "plt.xlim(0, 1000)\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, label=\"Train\")\n",
    "plt.plot(range(len(val_loss_list)), val_loss_list, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "def test():\n",
    "    test_net.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        total_max_temp_error = 0\n",
    "        total_min_temp_error = 0\n",
    "        max_error_x = 0\n",
    "        max_error_y = 0\n",
    "        min_error_x = 0\n",
    "        min_error_y = 0\n",
    "        error_per_pix = 0\n",
    "        loss_sum = 0\n",
    "        max_error_x_abs = 0\n",
    "        max_error_y_abs = 0\n",
    "        min_error_x_abs = 0\n",
    "        min_error_y_abs = 0\n",
    "            \n",
    "        estimate_list = []\n",
    "\n",
    "        for batch_idx, (x, c) in enumerate(test_loader):\n",
    "            estimate_start = time.time()\n",
    "            print('=================================')\n",
    "            x, c = tmp.next()\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            y = model(x)\n",
    "            estimate_end = time.time()\n",
    "            estimate_duration = -estimate_start+estimate_end\n",
    "            print(\"{}個目のモデルの推定時間は：{}秒です\".format(batch_idx+1, estimate_duration))\n",
    "            estimate_list.append(estimate_duration)\n",
    "            continue\n",
    "            loss = criterion(y, c)\n",
    "            loss_unit = torch.sum(torch.abs(y-c))/16/18\n",
    "            loss_sum = loss_sum + loss_unit\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            y = y.cpu()\n",
    "            c = c.cpu()\n",
    "            c_reshaped = c.reshape(16, 18)\n",
    "            seaborn.heatmap(c_reshaped, vmin=18, vmax=25)\n",
    "            plt.show()\n",
    "            y_reshaped = y.reshape(16, 18)\n",
    "            seaborn.heatmap(y_reshaped, vmin=18, vmax=25)\n",
    "            plt.show()\n",
    "            \n",
    "            #最高温度のずれ\n",
    "            max_temp_output = torch.max(c)\n",
    "            max_temp_target = torch.max(y)\n",
    "            max_temp_error = max_temp_output - max_temp_target\n",
    "            total_max_temp_error = total_max_temp_error + max_temp_error\n",
    "\n",
    "            #最低温度のずれ\n",
    "            min_temp_output = torch.min(c)\n",
    "            min_temp_target = torch.min(y)\n",
    "            min_temp_error = min_temp_output - min_temp_target\n",
    "            total_min_temp_error = total_min_temp_error + min_temp_error\n",
    "\n",
    "            #最高温度を示すピクセルの座標のずれ(絶対値)\n",
    "            max_temp_arg = torch.argmax(c)\n",
    "            max_x_output = (max_temp_arg + 1)%18 -1\n",
    "            max_y_output = -(max_temp_arg + 1)//18\n",
    "\n",
    "            max_temp_arg = torch.argmax(y)\n",
    "            max_x_target = (max_temp_arg + 1)%18 -1\n",
    "            max_y_target = -(max_temp_arg + 1)//18\n",
    "            \n",
    "            max_error_x = max_error_x + abs(max_x_output - max_x_target)\n",
    "            max_error_y = max_error_y + abs(max_y_output - max_y_target)\n",
    "\n",
    "            # 最低温度を示すピクセルの座標のずれ(絶対値)\n",
    "            min_temp_arg = torch.argmin(c)\n",
    "            min_x_output = (min_temp_arg + 1)%18 -1\n",
    "            min_y_output = -(min_temp_arg + 1)//18\n",
    "\n",
    "            min_temp_arg = torch.argmin(y)\n",
    "            min_x_target = (min_temp_arg + 1)%18 -1\n",
    "            min_y_target = -(min_temp_arg + 1)//18\n",
    "            \n",
    "            min_error_x = min_error_x + abs(min_x_output - min_x_target)\n",
    "            min_error_y = min_error_y + abs(min_y_output - min_y_target)\n",
    "\n",
    "    print(\"平均時間は{}秒です\".format(sum(estimate_list)/len(estimate_list)))\n",
    "    return 0\n",
    "\n",
    "    print('1ピクセルあたりの誤差', loss_sum/len(test_loader))\n",
    "    print('最高温度のずれ', total_max_temp_error/len(test_loader))      \n",
    "    print('最低温度のずれ', total_min_temp_error/len(test_loader))\n",
    "    print('最高温度を示すピクセルの座標のずれ', max_error_x_abs/len(test_loader), max_error_y_abs/len(test_loader), )\n",
    "    print('最低温度を示すピクセルの座標のずれ', min_error_x_abs/len(test_loader), min_error_y_abs/len(test_loader), )\n",
    "    \n",
    "    print(\"batch_size: {}　です。頑張ってください\".format(batchsize))\n",
    "    print(fpath)\n",
    "    p = [loss_sum/len(test_loader), total_max_temp_error/len(test_loader), total_min_temp_error/len(test_loader), max_error_x/len(test_loader), max_error_y/len(test_loader), min_error_x/len(test_loader), min_error_y/len(test_loader), max_error_x_abs/len(test_loader), max_error_y_abs/len(test_loader), min_error_x_abs/len(test_loader), min_error_y_abs/len(test_loader), best_epoch]\n",
    "    p = [float(x) for x in p]\n",
    "    p.insert(0, \"条件\"+str(histwindow))\n",
    "    print(*p, sep=', ')\n",
    "\n",
    "    val_loss = running_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac876c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
